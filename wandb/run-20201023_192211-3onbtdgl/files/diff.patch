diff --git a/markipy/nn/lr/model.py b/markipy/nn/lr/model.py
index 35fdd13..bf62c31 100644
--- a/markipy/nn/lr/model.py
+++ b/markipy/nn/lr/model.py
@@ -15,34 +15,43 @@ class LogisticRegrssion(nn.Module):
 
     def forward(self, x):
         x = self.rel(self.l1(x))
-        return  self.sig(self.l2(x))
+        return self.sig(self.l2(x))
 
 
 if __name__ == "__main__":
-    # 1. Start a new run
+    # Computational Device 
+    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
+
+    #Wb1. Start a new run
     wandb.init(project="lr")
 
-    # 2. Save model inputs and hyperparameters
+    #Wb2. Save model inputs and hyperparameters
     config = wandb.config
-    config.learning_rate = 0.001
-
-    epoch = int(1e3)
-    dim = 100
+    # Parameters
+    config.learning_rate = lr = 0.001
+    config.epoch = epoch = int(1e3)
+    config.input_dimension = dim = 100
 
+    # Data Input X and Y
     x = [x for x in range(dim)]
-    y = torch.FloatTensor([0, 1])
+    y = torch.FloatTensor([0, 1]).cuda()
 
+    # Pytorch Tensor
     tensor_x = torch.FloatTensor(x).view(dim, 1)
-    tensor_y = torch.FloatTensor(y).view(2, 1)
+    tensor_y = torch.FloatTensor(y).view(2, 1).cuda()
+    
+    # (CUda) Check  
+    tensor_x = tensor_x.to_device(device)
+    tensor_x = tensor_y.to_device(device)
 
     # Init model
-    model = LogisticRegrssion(1)
+    model = LogisticRegrssion(1).cuda()
 
-    # 3. Log gradients and model parameters
+    #Wb3. Log gradients and model parameters
     wandb.watch(model)
 
     # Cost Function
-    criterion = nn.BCELoss()
+    criterion = nn.BCELoss().cuda()
 
     # Optimizer
     optimizer = torch.optim.SGD(model.parameters(), lr=0.001)
diff --git a/requirements.txt b/requirements.txt
index b3491fd..5215906 100755
--- a/requirements.txt
+++ b/requirements.txt
@@ -18,5 +18,4 @@ python-gnupg
 tables
 torch~=1.6.0
 PyYAML~=5.3.1
-terminator~=1.92
-wandb~=0.10.7
\ No newline at end of file
+wandb~=0.10.7
