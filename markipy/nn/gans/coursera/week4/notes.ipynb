{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Unconditional generation\n",
    "As a quick recap with unconditional generation, you get outputs from random classes. You can think of those as a gumball\n",
    "machine, where you input a coin and you get a random color gumball. If you want to gumball of a specific color, say red,\n",
    "you have to keep spinning coins until you get it. In this example, the coin is like the random noise vector that your GAN\n",
    "uses for generation and the gumball machine is like the generator. Then the gumballs are the random outputs, those images\n",
    "you get. You can see what color gumballs you might get, just like how you know what your GAN is trained on.\n",
    "You can't control what exact color of output you will get.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Conditional generation\n",
    "On the other hand, conditional generation allows you to ask for an example from a specific class and you get what you ask for.\n",
    "It's like a vending machine. You input a coin similar to the gumball machine but you input a coin along with a code for an\n",
    "item that you want. For example, A2 for a red soda. But note that you still don't control certain features of the soda bottle.\n",
    "You can't get the one with the latest expiration date or the bottle that's least damaged or the one that's filled up the most,\n",
    "you just get a random red soda. But it is a red soda, not a blue candy bar. Here the coin and the code are the inputs for\n",
    "the GAN and the vending machine is the generator and the soda is the generated output. With a conditional GAN, you get a\n",
    "random example from the class you specify. That class is this A2 soda here.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Conditional vs Unconditional generation\n",
    "\n",
    "<img src=\"images/condVSuncond.png\" width=420 height=200 />\n",
    "\n",
    "With conditional generation, you can get generated examples from the classes you decide while with unconditional generation,\n",
    "you get examples from a random class. As a result of that, with conditional generation, you actually have to train your\n",
    "GAN with labeled datasets and those labels are on the different classes you want while unconditional generation doesn't\n",
    "need any labels. You've seen this in previous weeks from the course, that you don't need any labels you just need a pile\n",
    "of real examples. You see how to modify your model for this conditional generation in the following lectures.\n",
    "What you should take away from this video is that conditional generation requires labeled datasets for training in\n",
    "order to learn how to produce examples from desired classes. Coming up, I'll show you how the labels from your dataset\n",
    "are fed to the generator and discriminator in order to train your GAN and produce examples from the desired class, like\n",
    "selecting the red soda from a vending machine.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Conditional generation - INPUT\n",
    "You've seen already with unconditional generation, the generator needs a noise vector to produce random examples.\n",
    "For conditional generation, you also need a vector to tell the generator from which class the generated examples should come from.\n",
    "Usually this is a one-hot vector\n",
    "\n",
    "<img src=\"images/cond-input.png\" width=420 height=200 />\n",
    "\n",
    "There's one-hot vector is now what lets you control what class to generate. The input to the generator in a conditional\n",
    "GAN is actually a concatenated vector of both the noise and the one-hot class information.\n",
    "\n",
    "\n",
    "<img src=\"images/cond-input-1.png\" width=420 height=200 />"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "So it's one huge vector. For example, with a class vector here representing a husky breed, the generator will ideally\n",
    "produce a husky dogs from one noise vector. But if you change that noise vector, it should produce a different husky dog;\n",
    "while the class information stays the same. Well, that's actually because the discriminator will also be given this class\n",
    "information. The generator, needs to fool the discriminator; so you'll see how this happens.\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Discriminator - Input\n",
    "\n",
    "The discriminator in a similar way will take the examples, but now they're paired with the class information as input\n",
    "to determine if the examples are either real or fake representations of that particular class; and that's what's key here.\n",
    "For example, with this class of golden retriever, but in image of a beagle dogs.\n",
    "\n",
    "<img src=\"images/cond-input-2.png\" width=420 height=200 />\n",
    "\n",
    "So not a golden retriever being fed in. This discriminated arches actually say this is fake because that is not a real\n",
    "looking golden retriever, even though it's a very real looking image of a beagle.\n",
    "\n",
    "This is just 5 percent real is what it's going to say. As a result, for the discriminator to predict that an example is\n",
    "real, it needs to look like the examples from that class in the training data set. The training data-set is key here\n",
    "because it'll get these real labels next to it.\n",
    "\n",
    "\n",
    "Digging a little bit deeper, the input to the discriminator is an image. How do you go about adding that class information?\n",
    "Well, the image is fed in as three different channels,RGB, red, green, blue, or just one channel if it's a gray-scale image.\n",
    "\n",
    "<img src=\"images/cond-input-3.png\" width=420 height=200 />\n",
    "\n",
    "o that's this image component. Then the one-hot class information could also be fed in as additional channels where all\n",
    "the channels take on values of all zeros; so all these white blocks of the same height and width as the image take on\n",
    "values of all zeros. Whereas this black one here will take on values of ones. In contrast to that one-hot vector,\n",
    "these are typically much larger matrices where each channel is full of zeros at every position where it's not that class.\n",
    "There are many other less space consuming ways of doing this, like compressing this information in another format.\n",
    "\n",
    "###### Better approach\n",
    "Here are many other less space consuming ways of doing this, like compressing this information in another format.\n",
    "You can even create a separate neural network head to do that for you, which would be prudent if you had many different classes."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Summary\n",
    "\n",
    "<img src=\"images/cond-input-4.png\" width=420 height=200 />\n",
    "\n",
    "To sum up in conditional generation, you pass the class information to both models. To the generator it's typically a\n",
    "one-hot vector concatenated with your noise vector to the discriminator when the desired output of the GANs are images,\n",
    "it's one-hot matrices representing the channels. The size of the class vector and the number of extra channels for the\n",
    "class information is just the same as the number of classes you'll be turning on."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Controllable generation\n",
    "\n",
    "Another way to control the outputs produced by gans, is after it's been trained, or more generally, controllable generation.\n",
    "While conditional generation leverages labels during training, this video will focus on controlling what features you\n",
    "want in the output examples, even after the model has been trained. You'll learn about controlling specific features,\n",
    "and how it compares with conditional generation you learned about in the previous video. Controllable generation allows\n",
    "you to control some of the features that you want in your output examples.\n",
    "\n",
    "<img src=\"images/ctr-gen-1.png\" width=420 height=200 />\n",
    "\n",
    "For instance, with a gan that performs face generation, you could control the age of the person's looks in the image or\n",
    "if they have sunglasses or the direction they're looking at in the picture, or they're perceived gender."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can do this by actually tweaking the input noise vector Z, that is fed to the generator after you train the model.\n",
    "For example, with the input noise vector Z, maybe you get this picture of a woman with red hair.\n",
    "\n",
    "<img src=\"images/ctr-gen-2.png\" width=420 height=200 />\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's say you tweak one of the features from this input noise vector here and Maybe now you get the same woman but with\n",
    "blue hair this time. Maybe its because this first element here, represents changing hair color. That would be super cool.\n",
    "You'll learn exactly how to tweak the Z in the following lecture.\n",
    "\n",
    "<img src=\"images/ctr-gen-3.png\" width=420 height=200 />"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Controllable vs Conditional Generator\n",
    "\n",
    "But first, to get a better sense of control generation, I'll make a quick comparison with conditional generation.\n",
    "I'll use these terms here because that's typically what researchers mean if you check out the papers, though it's not as\n",
    "clearly delineated\n",
    "\n",
    "<img src=\"images/ctr-gen-4.png\" width=420 height=200 />\n",
    "\n",
    "Sometimes controlable degeneration can definitely include conditional generation because you're still controlling the\n",
    "gan in some way. With controllable generation, you're able to get examples with the features that you want, like faces\n",
    "from people who look older with green hair and glasses. With conditional generation, you get examples from the class that\n",
    "you want, like a human or bird. Of course, it could also be, I want to person with sunglasses on as well. So far, they're\n",
    "a bit similar. But controllable generation typically means you want to control how much or how little of a feature you want.\n",
    "They're typically more continuous features like age. Conditional generation on the other hand, allows you to specify what\n",
    "class you want to a very different type of thing. For this, you need to have a label data set and implemented during\n",
    "training typically. You probably don't want to label every hair length value, so, controllable generation will do that\n",
    "for you and it's more about finding directions of the features you want. That can happen after training. Of course,\n",
    "controllable generation, you will sometimes also see it happening during training as well. To help nudge the model in\n",
    "a direction where it's easier to control. Finally, as you just learned, controllable generation works by tweaking that\n",
    "input noise vector Z that's fed into the generator, while with conditional generation, you have to pass additional\n",
    "information representing the class that you want appended to that noise vector."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-7a2079c2aa84>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;36m  File \u001B[0;32m\"<ipython-input-1-7a2079c2aa84>\"\u001B[0;36m, line \u001B[0;32m3\u001B[0m\n\u001B[0;31m    But first, to get a better sense of control generation, I'll make a quick comparison with conditional generation. I'll use these terms here because that's typically what researchers mean if you check out the papers, though it's not as clearly delineated\u001B[0m\n\u001B[0m        ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m invalid syntax\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "In summary. Controllable generation lets you control the features in the output from your gan.\n",
    "In contrast, with conditional generation, there's no need for a labeled training dataset.\n",
    "To change the output in some way with controllable generation, the input noise vector is tweaked in some other way.\n",
    "In following videos, I'll dig deeper into how exactly that works.\n",
    "\n",
    "<img src=\"images/ctr-gen-5.png\" width=420 height=200 />"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Vector Algebra in the Z-Space\n",
    "\n",
    "<img src=\"images/ctr-gen-6.png\" width=420 height=200 />\n",
    "\n",
    "As you've seen in the previous video, controllable generation is achieved by manipulating the noise vector z that's fed\n",
    "into the generator. In this video, I'll show you the intuition behind that process. I'll review how to interpolate\n",
    "between two GAN outputs first and you'll learn how to manipulate noise vectors in order to control desired features in\n",
    "your outputs.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"images/ctr-gen-7.png\" width=420 height=200 />\n",
    "\n",
    "Controllable generation and interpolation are somewhat alike. With interpolation, you get these intermediate examples\n",
    "between two generated observations. In practice, with interpolation, you can see how an image morphs into another,\n",
    "like in this GIF, where each digit from zero to nine morphs into the following one."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"images/ctr-gen-8.png\" width=420 height=200 />\n",
    "\n",
    "What happens is that you get intermediate examples between the targets by manipulating the inputs from Z-space,\n",
    "which is just the name for the vector space of the noise vectors. You'll see later that this is the same idea behind\n",
    "control bot generation. Just to be clear here, Z_1 and Z_2 are the two dimensions in this Z-space that you're looking\n",
    "at right now."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Example Latent Z Space\n",
    "\n",
    "<img src=\"images/ctr-gen-9.png\" width=420 height=200 />\n",
    "\n",
    "As an example, there's a noise vector V_1 and a noise vector V_2, where V_1 could have a Z_1 value of, let's say five\n",
    "and a Z_2 value of let's say 10. Then this is the vector 5,10 and then V_2 has a smaller value, so four and two.\n",
    "So it's the vector 4, 2. That's what Z_1 and Z_2 are, just dimensions on the Z-space and the actual vectors V_1 and\n",
    "V_2 are going to represent concrete vector values in this Z-space. V_1, when you feed it into the generator, will\n",
    "produce this image here, and V_2 when you feed it into the generator, will produce this image there."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Interpolation Latent Space Z\n",
    "\n",
    "<img src=\"images/ctr-gen-10.png\" width=420 height=200 />\n",
    "\n",
    "\n",
    "If you want to get intermediate values between these two images, you can make an interpolation between their two\n",
    "input vectors, V_1 and V_2 in the Z-space, actually. This interpolation is often a linear interpolation.\n",
    "Of course, there are other ways to interpolate between these two vectors."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"images/ctr-gen-11.png\" width=420 height=200 />\n",
    "\n",
    "Then you can take all these intermediate vectors and see what they produce from the generator.\n",
    "The generator takes this vector and produces that image, this vector, that image, and this vector, that image\n",
    "to get this gradient between these two images.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"images/ctr-gen-12.png\" width=420 height=200 />\n",
    "\n",
    " Controllable generation also uses changes in the Z-space and takes advantage of how modifications to the noise vectors\n",
    " are reflected on the output from the generator. For example, with the noise vector, you could get a picture of a woman\n",
    " with red hair and then with another noise vector, you could get a picture of the same woman but with blue hair.\n",
    " The difference between these two noise vectors is just this direction in which you have to move in Z-space to modify\n",
    " the hair color of your generated images. In controllable generation, your goal is to find these directions for\n",
    " different features you care about. For example, modifying hair color. But don't worry about finding that exact\n",
    " direction yet, I'm going to show you in the following lectures. With this known direction d, let's call this\n",
    " direction d, in your Z-space, you can now control the features on the output of your GAN, which is really exciting."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"images/ctr-gen-13.png\" width=420 height=200 />\n",
    "\n",
    "his means that if you then generate an image of a man with red hair produced by the same Generator g, with this input\n",
    "noise vector here, V_1, you can modify the hair color of this man in the image by adding that direction vector d you\n",
    "found earlier to the noise vector, creating this new noise vector here, V_1 + d, passing that into your generator and\n",
    "getting a resulting image where his hair is now blue.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"images/ctr-gen-14.png\" width=420 height=200 />\n",
    "\n",
    "To sum up, in controllable generation, you need to find the directions in the Z-space related to changes of the desired\n",
    "features on the output of your GAN. With known directions, controllable generation works by moving the noise vector in\n",
    "different directions in that Z-space. Up next, you'll learn some challenges related to controllable generation and how\n",
    "to find directions on the Z-space with known effects on the generated outputs."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Challenges with Controllable Generation\n",
    "\n",
    "<img src=\"images/ctr-gen-15.png\" width=420 height=200 />\n",
    "\n",
    "Controllable generation makes it so that you can decide the features and the output of a GAN, like hair color or hair\n",
    "length in the GAN that produces pictures of people. However, controllable generation has a couple of challenges that\n",
    "you'll see in this video. Specifically, you'll learn about feature correlation and the alpha space in Z space entanglement.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Feature correlation\n",
    ". When different features have a high correlation in the data set, they use to train your GAN, it becomes difficult to\n",
    "control specific features without modifying the ones that are correlated to them. For example, ideally you want to be\n",
    "able to control single features like the amount of beard on a person's face produced by your GAN and if features in the\n",
    "data set don't happen to have a high correlation, you'd be able to take this picture of a woman and add a beard to her\n",
    "by moving in some direction in Z space.\n",
    "\n",
    "<img src=\"images/ctr-gen-16.png\" width=420 height=200 />\n",
    "\n",
    ". However, it's very likely that in the data set you use for training features like the presence of a beard and how\n",
    "masculine the face looks will be strongly correlated. If you want to add a beard to the picture of a woman, you'd\n",
    "end up modifying many more features on the output. But that perhaps is not desirable because you want to be able\n",
    "to find these directions where you can just change one feature about someone. That way you can reliably edit images."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Z-Space Entanglement\n",
    "Another challenge faced by controllable generation is known as entanglement in the Z space. When the Z space is entangled,\n",
    "movement in the different directions has an effect on multiple features simultaneously in the output.\n",
    "\n",
    "<img src=\"images/ctr-gen-17.png\" width=420 height=200 />\n",
    "\n",
    "Even if those features aren't necessarily correlated in your training data set. This is just how the noise space was learned;\n",
    "to be very entangled. In this entangled Z space, when you control if a person in the output has glasses, for example,\n",
    "you also end up modifying whether she has a beard in her hair or when you try to modify her apparent age, you'll end up\n",
    "also changing her apparent eyes and hair color too. That's not quite desirable. The same happens with other uncorrelated\n",
    "features. This means that changes in some of the components of the noise vectors change multiple features in the output\n",
    "at the same time and this makes it difficult, if not impossible, to control the output. This is a very common problem\n",
    "when the Z space doesn't have enough dimensions relative to the number of features you want to control in the output\n",
    "because then it actually can't map things one-to-one. This is also just generally an issue when training generative models.\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"images/ctr-gen-18.png\" width=420 height=200 />\n",
    "\n",
    "To recap, controllable generation faces several challenges. If features in your data set has a high correlation with each\n",
    "other and you don't account for this in some way, then when you try to control the output of your GAN, you end up changing\n",
    "multiple features at a time. Even if the features you want to control don't have a high correlation with each other in\n",
    "the training data set, control of a generation is also difficult if your Z space is entangled. Which will happen commonly\n",
    "if the number of dimensions in your Z space is not large enough but there are a host of other reasons of why that happens\n",
    "as well.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Classifier Gradients\n",
    "\n",
    "<img src=\"images/ctr-gen-19.png\" width=420 height=200 />\n",
    "\n",
    "Trouble generation works by moving into the z-space, according to directions that correspond to desired features, such as\n",
    "lengthening hair or shortening hair. In this video, you'll learn a very simple method used to find that direction using\n",
    "the gradient of trained classifiers, you'll see what the requirements are for this method.\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "So to find the direction in the z-space that modifies certain features on the output, say the presence of sunglasses.\n",
    "You could use a train classifier that identifies if a person in a picture has that said feature. So to do so, you\n",
    "could take a batch of noise vector Z, that goes through the generator to get some images. You then pass these images\n",
    "through a sunglasses classifier, which will tell you at the outputs correspond to people with or without sunglasses.\n",
    "Then you use them information to modify your Z vectors, and this is without modifying the weights of the generator at\n",
    "all. So the generator weights are frozen. You're done training, and you modify your Z vectors by moving in the\n",
    "direction of the gradient with the costs. That penalizes the model for every image classified as not having sunglasses.\n",
    "So then you repeat this process until the images are classified as people with sunglasses.\n",
    "\n",
    "<img src=\"images/ctr-gen-20.png\" width=420 height=200 />\n",
    "\n",
    "So this method is very simply inefficient, and some might argue, even lazy, because you're using this classifier that's\n",
    "already there for you. But I think it's great, that you're taking advantage of a pre-training classifier. However, of\n",
    "course there's also the downside of that. You need a pre-trained classifier that accurately detects the feature that\n",
    "you want to control with four hand. You can also train your own of course, so if you wanted to classifier that detected\n",
    "beards, for example, you might have to train that on your own if that isn't available off the shelf. Be you should always\n",
    "check if something is available out there, because this could be a really simple and cool way to allow you to start\n",
    "controlling your."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"images/ctr-gen-21.png\" width=420 height=200 />\n",
    "\n",
    "So you should take away from this video that preacher classifiers can be used to find directions in the z-space associated\n",
    "with features in the output of. And to find those directions using the gradients of the classifiers, you need to modify\n",
    "the noise vectors without changing the generator. So remember all of this happens after training.\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Disentanglement\n",
    "\n",
    "<img src=\"images/ctr-gen-22.png\" width=420 height=200 />\n",
    "\n",
    "Previously, you saw what entanglement and the z-space means and why that's problematic in controllable generation.\n",
    "In this video, I'll talk about disentanglement and some ways to encourage your model to achieve it. First,\n",
    "I'll revisit what entangled versus disentangled z-space means, then I'll mention some of the most popular\n",
    "ways to encourage your model to have a disentangled z-space.\n",
    "\n",
    "\n",
    "<img src=\"images/ctr-gen-23.png\" width=420 height=200 />\n",
    "\n",
    "Take the following z-noise vectors, v_1 and v_2, and imagine that these are coming from a disentangled z-space.\n",
    "Now, here on the right, I'm visualizing two dimensions of that z-space, but, of course, as you can tell from these\n",
    "noise vectors, there's many more than just two dimensions. Now, if this is a disentangled z-space, then each of these\n",
    "positions would correspond to a single feature in the output. For example, this first element, this first dimension of\n",
    "the noise vector would correspond to hair color, and the second dimension would correspond to hair length, and so on.\n",
    "\n",
    "\n",
    "<img src=\"images/ctr-gen-24.png\" width=420 height=200 />\n",
    "\n",
    "If you wanted to change the hair color in a picture, you would literally just change the first element on its noise vector,\n",
    "or move in the z_1 direction on that z-space; and that's all you have to do, change the hair color. Then for hair length,\n",
    "you just move in the z_2 dimension. That's just changing these values here in the second position. Also, it's quite possible\n",
    "that these noise vectors are disentangled with respect to just these two different features and the rest of the values\n",
    "don't mean anything. This is because you typically want the size of your noise vector to be larger than the number of\n",
    "features you want to control. This makes it easier for your model to learn because it allows these values to take on\n",
    "different things over time during training. For example, if you only wanted to control these two features like hair\n",
    "color and hair length, then it's probably prudent to make your noise vector larger than just two-dimensions, but here\n",
    "you see three or more. These other values won't control a specific feature, they just help the model adapt to in training.\n",
    "Because the components of the noise vectors in the disentangled z-space allow you to change those features that you desire\n",
    "in the output, they're often called latent factors of variation, where the word latent comes from the fact that the\n",
    "information from the noise vectors is not seen directly on the output, but they do determine how that output looks.\n",
    "\n",
    "\n",
    "Sometimes you might hear noise vectors being referred more generally as latents. Then factors of variation means that\n",
    "these are just different factors like hair color and hair length that you want to vary, and only that one factor,\n",
    "that one feature that you are varying when you're varying it, not anything else.\n",
    "\n",
    "\n",
    "<img src=\"images/ctr-gen-25.png\" width=420 height=200 />\n",
    "\n",
    "Essentially, a disentangled z-space means that there are specific indices on the noise vectors, these specific dimensions\n",
    "that change particular features on the output of your GAN. For instance, if a person on the image generated by a GAN has\n",
    "glasses or not, or whether she has a beard or some features of her hair. Each of these cells will correspond to something\n",
    "that you desire to change, and you can just change the values of that dimension in order to adapt glasses, beard, or hair.\n",
    "A crucial different between a disentangled z-space and an entangled z-space is that, here, with a disentangled z-space,\n",
    "when you control one of the features on the output, for example, glasses, the other features remain the same. The beard\n",
    "and hair will remain the same. Or if I change whether someone has a beard, the glasses and hair will stay the same.\n",
    "What this means is that with a disentangled z-space, you're much more likely to be able to add a beard to a person\n",
    "that looks very feminine without changing her hair or her facial features.\n",
    "\n",
    "\n",
    "#### Encourage Disentanglement: Supervision\n",
    "\n",
    "<img src=\"images/ctr-gen-26.png\" width=420 height=200 />\n",
    "\n",
    "\n",
    "One way to encourage your model to use disentangled z-spaces is to label your data and follow a similar process,\n",
    "the one used for conditional generation. But in this case, the information from the class is embedded in the noise vector;\n",
    "so you don't need this extra one-hot class information or class vector. However, using this method could be problematic\n",
    "for continuous classes. Imagine having to label thousands of human faces with the length of their hair. Of course,\n",
    "if you do do this, even having a few different classes, a few different buckets, might actually nudge your generator\n",
    "in the right direction.\n",
    "\n",
    "\n",
    "<img src=\"images/ctr-gen-27.png\" width=420 height=200 />\n",
    "\n",
    "Another way to encourage your model is to use a disentangled z-space without labeling any of your examples. Instead,\n",
    "you add a regularization term to the loss function of your choice like BCE or W-loss to encourage your model to associate\n",
    "each index from the noise vectors, two different features on the output. This regularization could come from classifier\n",
    "gradients and they're also much more advanced techniques to do this in an unsupervised way, meaning without any labels.\n",
    "\n",
    "<img src=\"images/ctr-gen-28.png\" width=420 height=200 />\n",
    "\n",
    " In summary, disentangled z-spaces let you control individual output features by corresponding certain z-values directly\n",
    " to desired features you want to control. To encourage your model to use disentangled noise vectors, you can use both\n",
    " supervised and unsupervised learning methods."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-d819d301",
   "language": "python",
   "display_name": "PyCharm (MarkPy)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}