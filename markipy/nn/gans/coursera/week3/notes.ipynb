{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Week 3\n",
    "Here we are going to see some problem relative the GANS Learning collapse and then how to resolve them."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Probability Distribution - Modes\n",
    "Usually our goal for the **generator** is to learn the *probability distribution* of the data and for the discriminator\n",
    "is to learn distinguish data real or fake.\n",
    "\n",
    "The probability distribution can have one mode or multiple modes:\n",
    "\n",
    "<img src=\"images/modes.png\" width=420 height=200 />\n",
    "\n",
    "##### Generator Problem\n",
    "The problem of the generator is that if in the train-data present multiple modes and the end can focus to learn only one\n",
    "of them and then keep generating always the same class."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Problems with BCE Loss\n",
    "\n",
    "Binary Cross-Entropy loss or BCE loss, is traditionally used for training GANs, but it isn't the best way to do it.\n",
    "With BCE loss GANs are prone to mode collapse and other problems. In this video, you'll see why GANs trained with BCE\n",
    "loss are susceptible to vanishing gradient problems. To that end, you'll review the BCE loss function and what that means\n",
    "for the generator and the discriminators objectives. Then you'll see when and why GANs with this BCE loss are likely\n",
    "to have those vanishing gradient problems.\n",
    "\n",
    "<img src=\"images/bce.png\" width=420 height=200 />"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Remember the form of the BCE loss function, its just an average of the cost\n",
    "for the discriminator for misclassifying real and fake observations. Where the first term is for reals and the second\n",
    "term is for the fakes. The higher this cost value is, the worse the discriminator is doing at it. The generator\n",
    "wants to maximize this cost because that means the discriminator is doing poorly and is classifying it's fake values\n",
    "into reals. Whereas the discriminator wants to minimize this cost function because that means it's classifying\n",
    "things correctly. Of course the generator only sees the fake side of things, so it actually doesn't see anything\n",
    "about the reals. This maximization and minimization is often called a minimax game, and that's how you might hear\n",
    "it being referred to as.\n",
    "\n",
    "<img src=\"images/bce2.png\" width=420 height=200 />"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n",
     "is_executing": true
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-4-d958f9b7e511>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;36m  File \u001B[0;32m\"<ipython-input-4-d958f9b7e511>\"\u001B[0;36m, line \u001B[0;32m1\u001B[0m\n\u001B[0;31m    Remember the form of the BCE loss function, its just an average of the cost\u001B[0m\n\u001B[0m             ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m invalid syntax\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "At the end of this minimax game, the generator and discriminator interaction translates to\n",
    "a more general objective for the whole GAN architecture. That is to make the real in generated data distributions\n",
    "of features very similar. Trying to get the generated distribution to be as close as possible to the reals.\n",
    "This minimax of the Binary Cross-Entropy loss function is somewhat approximating the minimization of another\n",
    "complex hash function that's trying to make this happen. Of course, during this whole training process, the\n",
    "discriminator naturally is trying to delineate this real and fake distribution as much as possible, whereas\n",
    "the generator is trying to make the generated distribution look more like the reals.\n",
    "\n",
    "<img src=\"images/gan.png\" width=420 height=200 />"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "However, let's take a\n",
    "step back again to the generator and discriminators roles. The discriminator and again, needs to output just\n",
    "a single value prediction within zero and one. Whereas the generator actually needs to produce a pretty complex\n",
    "output composed of multiple features to try and fool the discriminator, for example, an image. As a result\n",
    "that discriminators job tends to be a little bit easier. To put it in another way, it's more straightforward\n",
    "to look at images in a museum than it is to paint those masterpieces, right? During training it's possible\n",
    "for the discriminator to outperform the generator, very possible, in fact, quite common.\n",
    "\n",
    "<img src=\"images/lossgan.png\" width=420 height=200 />\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "But at the beginning\n",
    "of training, this isn't such a big problem because the discriminator isn't that good. It has trouble\n",
    "distinguishing the generated and real distributions.There's some overlap and it's not quite sure. As a result, it's able\n",
    "to give useful feedback in the form of a non-zero gradient back to the generator.\n",
    "\n",
    "<img src=\"images/begintrain.png\" width=420 height=200 />\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "However, as it gets better at training, it starts to delineate the generated and real\n",
    "distributions a little bit more such that it can start distinguishing them much more. Where the real distribution will\n",
    "be centered around one and the generated distribution will start to approach zero. As a result, when it's starting to\n",
    "get better, as this discriminator is getting better, it'll start giving less informative feedback. In fact, it might\n",
    "give gradients closer to zero, and that becomes unhelpful for the generator because then the generator doesn't know how\n",
    "to improve. This is how the vanishing gradient problem will arise.\n",
    "\n",
    "<img src=\"images/aftertrain.png\" width=420 height=200 />"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In summary, GANs try to make the generated\n",
    "distribution look similar to the real one by minimizing the underlying cost function that measures how different the\n",
    "distributions are. As a discriminator improves during training and sometimes improves more easily than the generator,\n",
    "that underlying cost function will have those flat regions when the distributions are very different from one another,\n",
    "where the discriminator is able to distinguish between the reals and the fakes much more easily, and be able to say,\n",
    "\"Reals look really real, a label of one and fakes look really fake, a label of zero.\" All of this will cause vanishing\n",
    "gradient problems.\n",
    "\n",
    "<img src=\"images/end.png\" width=420 height=200 />"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Earth Mover's Distance\n",
    "\n",
    "When using BCE loss to train a GAN, you often encounter mode collapse, and vanishing gradient problems due to the underlying\n",
    "cost function of the whole architecture. Even though there is an infinite number of decimal values between zero and one,\n",
    "the discriminator, as it improves, will be pushing towards those ends. In this video, you'll see a different underlying\n",
    "cost function called Earth mover's distance, that measures the distance between two distributions and generally outperforms\n",
    "the one associated with BCE loss for training GANs. At the end I'll show you why this helps with the vanishing gradient\n",
    "problem.\n",
    "\n",
    "<img src=\"images/emd.png\" width=420 height=200 />"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "So take this generated and real distributions with the same variance but different means, and assume they might\n",
    "be normal distributions. What the Earth Mover's distance does, is it measures how different these two distributions\n",
    "are, by estimating the amount of effort it takes to make the generated distribution equal to the real.\n",
    "\n",
    "<img src=\"images/emd1.png\" width=420 height=200 />\n",
    "\n",
    "\n",
    "So intuitively, the generate distribution was a pile of dirt, how difficult would it be to move that pile of dirt\n",
    "and mold it into the shape and location of the real distribution? So that's what this Earth mover's distance means.\n",
    "The function depends on both the distance and the amount that the generated distribution needs to be moved.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "So the problem with BCE loss is that as a discriminator\n",
    "improves, it would start giving more extreme values between zero and one, so values closer to one and closer to zero.\n",
    "As a result, this became less helpful feedback back to the generator. So the generator would stop learning due to\n",
    "vanishing gradient problems.\n",
    "\n",
    "<img src=\"images/emd4.png\" width=420 height=200 />\n",
    "\n",
    "With Earth mover's distance, however, there's no such ceiling to the zero and one.\n",
    "So the cost function continues to grow regardless of how far apart these distributions are. The gradient of this measure\n",
    "won't approach zero and as a result, GANs are less prone to vanishing gradient problems and from vanishing gradient\n",
    "problems, mode collapse."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"images/emd4.png\" width=420 height=200 />\n",
    "\n",
    "So wrapping up, Earth mover's distance is a function of the effort to make a distribution\n",
    "equal to another. So it depends on both distance and amount. Unlike BCE, it doesn't have flat regions when the\n",
    "distributions start to get very different, and the discriminator starts to improve a lot. So approximating this\n",
    "measure eliminates the vanishing gradient problem, and reduces the likelihood of mode collapse in GANs.\n",
    "In the next few videos, I'll show you a loss function that uses Earth mover's distance for training GANs."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Wassertein Loss - Approximation of EMD\n",
    "\n",
    "As you've seen previously, BCE Loss is used traditionally to train GANs. However, it has many problems due the form of\n",
    "the function it's approximated by. So in this video I'll introduce you to an alternative loss function called\n",
    "Wasserstein Loss, or W-Loss for short, that approximates the Earth Mover's Distance that you saw in the previous video.\n",
    "So to that end, first you'll see an alternative way to look at the BCE Loss function that's more simple and compact,\n",
    "and I'll show you how W-Loss is calculated, and I'll compare this loss with BCE Loss. So, BCE Loss is computed by a\n",
    "long equation that essentially measures how bad, on average, some observations are being classified by the discriminator,\n",
    "as fake and real.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"images/bce_simple.png\" width=420 height=200 />\n",
    "\n",
    "So, the generator in GANs wants to maximize this cost, because that means the discriminator is saying that its fake\n",
    "values seem really real, while the discriminator wants to minimize that cost.\n",
    "And so, this is often referred to as a Minimax game. And this very long equation for BCE Loss can be simplified as follows.\n",
    "The sum and division over examples M is nothing but a mean or expected value. In the first part, inside the sum, measures\n",
    "how bad the discriminator classifies real observations, where y equals 1, and 1 means real. And the second part measures\n",
    "how bad it classifies fake observations produced by the generator, where y of 1 means real, but 1 minus y, y of 0,\n",
    "means fake."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### W-Loss\n",
    "\n",
    "W-Loss, on the other hand, approximates the Earth Mover's Distance between the real and generated distributions,\n",
    "but it has nicer properties than BCE. However, it does look very similar to the simplified form for the BCE Loss, and in\n",
    "this case the function calculates the difference between the expected values of the predictions of the discriminator.\n",
    "Here it's called the critic, and I'll go over that later, so I'm going to represent it with a c here. And this is c of a\n",
    "real example x, versus C of a fake example g of z. Generator taking in a noise vector to produce a fake image g of z,\n",
    "or perhaps you can call it x-hat.\n",
    "\n",
    "<img src=\"images/wloss.png\" width=420 height=200 />"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "So the discriminator looks at these two things, and it wants to maximize the distance between its thoughts on the reals\n",
    "versus its thoughts on the fakes. So it's trying to push away these two distributions to be as far apart as possible.\n",
    "Meanwhile, the generator wants to minimize this difference, because it wants the discriminator to think that its fake\n",
    "images are as close as possible to the reals. I know that in contrast with BCE there are no logs in this function,\n",
    "since the critics outputs are no longer bounded to be between 0 and 1.\n",
    "\n",
    "<img src=\"images/wloss1.png\" width=420 height=200 />"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Discriminator ouput\n",
    "So, for the **BCE Loss** to make sense, the output of the discriminator needs to be a prediction between 0 and 1. And so the\n",
    "discriminator's neural network for GANs, trained with BCE Loss, have a sigmoid activation function in the output layer\n",
    "to then squash the values between 0 and 1\n",
    "\n",
    "<img src=\"images/disc-bce.png\" width=420 height=200 />"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**W-Loss**, however, doesn't have that requirement at all, so you can actually have a linear layer at the end of the\n",
    "discriminator's neural network, and that could produce any real value output. And you can interpret that output as,\n",
    "how real an image is considered by the critic, which, by the way, is now what we're calling the discriminator instead,\n",
    "because it's no longer bounded between 0 and 1, where 0 means fake, and 1 means real. It's no longer classifying into\n",
    "these two, or discriminating between these two classes. And so, as a result, it wouldn't make that much sense to call\n",
    "that neural network a discriminator, because it doesn't discriminate between the classes. And so, for W-Loss, the\n",
    "equivalent to a discriminator is called a critic, and what it tries to do is, maximize the distance between its\n",
    "evaluation on a fake, and its evaluation on a real.\n",
    "\n",
    "<img src=\"images/disc-wloss.png\" width=420 height=200 />"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### W-Loss vs BCE Loss\n",
    "\n",
    "So, some of the main differences between W-Loss and BCE Loss is that, the discriminator under BCE Loss outputs a value\n",
    "between 0 and 1, while the critic in W-Loss will output any number. Additionally, the forms of the cost functions is\n",
    "very similar, but W-Loss doesn't have any logarithms within it, and that's because it's a measure of how far the\n",
    "prediction of the critic for the real is from its prediction on the fake. Meanwhile, BCE Loss does measure that distance\n",
    "between fake or a real, but to a ground truth of 1 or 0. And so what's important to take away here is largely that, the\n",
    "discriminator is bounded between 0 and 1, whereas the critic is no longer bounded ,and just trying to separate the two\n",
    "distributions as much as possible.\n",
    "\n",
    "<img src=\"images/bce-vs-wloss.png\" width=420 height=200 />\n",
    "\n",
    "And as a result, because it's not bounded, the critic is allowed to improve without degrading its feedback back to the\n",
    "generator. And this is because, it doesn't have a vanishing gradient problem, and this will mitigate against mode\n",
    "collapse, because the generator will always get useful feedback back."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### W-Loss Summary\n",
    "\n",
    "<img src=\"images/wloss-summary.png\" width=420 height=200 />\n",
    "\n",
    "\n",
    "So, in summary, W-Loss looks very similar to BCE Loss, but it isn't as complex a mathematical expression.\n",
    "Under the hood what it does is, approximates the Earth Mover's Distance, so it prevents mode collapse in vanishing\n",
    "gradient problems. However, there is an additional condition on this cost function for it to work well and for it to be\n",
    "valid."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Wasserstein Loss or W-Loss\n",
    "\n",
    "Wasserstein Loss or W-Loss solves some problems faced by GANs, like mode claps and vanishing gradients. But for it to\n",
    "work well, there is a special condition that needs to be met by the critic. In this video, you'll see what the continuity\n",
    "condition on the critic neural network means and why that condition is important when using W-Loss for training GANs,\n",
    "and trust me, it's worth it, so stay tuned.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Condition to use W-Loss -> 1-Lipschitz Continuous (1-L)\n",
    "\n",
    "W-Loss is a simple expression that computes the difference between the expected values of the critics output for the\n",
    "real examples x and its predictions on the fake examples g(z). The generator tries to minimize this expression, trying\n",
    "to get the generative examples to be as close as possible to the real examples while the critic wants to maximize this\n",
    "expression because it wants to differentiate between the reals and the fakes, it wants the distance to be as large as\n",
    "possible. However, for training GANs using W-Loss, the critic has a special condition. It needs to be something called\n",
    "1-Lipschitz Continuous or 1-L Continuous for short.\n",
    "\n",
    "<img src=\"images/wloss-1L.png\" width=420 height=200 />"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This condition sounds more sophisticated than it really is. For a function like the critics neural network to be at\n",
    "1-Lipschitz Continuous, the norm of its gradient needs to be at most one. What that means is that, the slope can't be\n",
    "greater than one at any point, its gradient can't be greater than one. To check if a function here, for example, this\n",
    "function you see here, f(x) equals x squared, is 1-Lipschitz Continuous,\n",
    "\n",
    "<img src=\"images/wloss-1L-1.png\" width=420 height=200 />\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "you want to go along every point in this function and make sure its slope is less than or equal to one, or its gradient\n",
    "is less than or equal to one, and what you can do is, you can actually draw two lines, one where the slope is exactly\n",
    "one at this certain point that you're evaluating function, and one where the slope is negative one where you're\n",
    "evaluating our function. You want to make sure that the growth of this function never goes out of bounds from these\n",
    "lines because staying within these lines means that the function is growing linearly.\n",
    "\n",
    "<img src=\"images/wloss-1L-2.png\" width=420 height=200 />"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here this function is not Lipschitz Continuous because it's coming out in all these sections.\n",
    "It's not staying within this green area, which suggests that it's growing more than linearly.\n",
    "\n",
    "<img src=\"images/wloss-1L-3.png\" width=420 height=200 />\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Look at another example here. This is a smooth curve functions. You want to again check every single point on this\n",
    "function before you can determine whether or not that this is 1-Lipschitz Continuous.\n",
    "\n",
    "<img src=\"images/wloss-1L-4.png\" width=420 height=200 />"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here it looks fine, function looks good. Here it also looks good, here looks good. Let's say you take every single value\n",
    "and the function never grows more than linearly. This function is 1-Lipschitz Continuous.\n",
    "This condition on the critics neural network is important for W-Loss because it assures that the W-Loss function is not\n",
    "only continuous and differentiable, but also that it doesn't grow too much and maintain some stability during training.\n",
    "This is what makes the underlying Earth Movers Distance valid, which is what W-Loss is founded on. This is required for\n",
    "training both the critic and generators neural networks and it also increases stability because the variation as the\n",
    "GAN learns will be bounded.\n",
    "\n",
    "<img src=\"images/wloss-1L-5.png\" width=420 height=200 />"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To recap, the critic, and again that uses W-Loss for training needs to be 1-Lipschitz Continuous in order for its\n",
    "underlying Earth Mover's Distance comparison between the reals and the fakes to be a valid comparison. In order to\n",
    "satisfy or try to satisfy this condition during training, there are multiple different methods. Next, we'll learn about\n",
    "a couple of these methods.\n",
    "\n",
    "<img src=\"images/wloss-1L-6.png\" width=420 height=200 />"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1-Lipschitz Continuity Enforce\n",
    "\n",
    "One Lipschitz continuity or 1-L continuity of the critic neural network in your Wasserstein loss and gain ensures that\n",
    "Wasserstein loss is valid. You already saw what this means and this video, I'll show you how to enforce this condition\n",
    "when training your critic. First, I'll introduce you to two different methods to enforce 1-L continuity on the critic,\n",
    "namely weight clipping and gradient penalty."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1-Lipschitz\n",
    "\n",
    "<img src=\"images/wloss-1L-7.png\" width=420 height=200 />\n",
    "\n",
    "Then I'll discuss the advantages of gradient penalty over weight clipping. First recall that the critic being 1-L\n",
    "continuous means that the norm of its gradient is at most one at every single point of this function. This upside down\n",
    "triangle is assigned for gradient, this is the function, perhaps F is the critic here and X is the image.\n",
    "This just represents the norm of that gradient being less than or equal to one. Using the L2 norm is very common here,\n",
    "which just means its Euclidean distance or often thought of as your triangle distance of your hypotenuse. This is the\n",
    "distance between these two points not going this direction. It's this hypotenuse. Intuitively in two-dimensions,\n",
    "it's that the slope is less than or equal to one. At every single point of this function, it'll remain within these\n",
    "green triangles."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Weight Clipping\n",
    "\n",
    "<img src=\"images/wloss-1L-8.png\" width=420 height=200 />\n",
    "\n",
    "With weight clipping, the weights of the critics neural network are forced to take values between a fixed interval.\n",
    "After you update the weights during gradient descent, you actually will clip any weights outside of the desired interval.\n",
    "Basically what that means is that weights over that interval, either too high or too low, will be set to the maximum or\n",
    "the minimum amount allowed. That's clipping the weights there. This is one way of enforcing the 1-L continuity, but it\n",
    "has a way to downside. Forcing the weights of the critic to a limited range of values could limit the critics ability to\n",
    "learn and ultimately for the gradient to perform because if the critic can't take on many different parameter values,\n",
    "it's weights can't take on many different values, it might not be able to improve easily or find good loop optimal for\n",
    "it to be in. Not only is this trying to do 1-L continuity enforcement, this might also limit the critic too much.\n",
    "Or on the other hand, it might actually limit the critic too little if you don't clip the weights enough. There's a lot\n",
    "of hyperparameter tuning involved."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Gradient Penalty\n",
    "\n",
    "<img src=\"images/wloss-1L-9.png\" width=420 height=200 />\n",
    "\n",
    "The gradient penalty, which is another method, is a much softer way to enforce the critic to be one lipschitz continuous.\n",
    "With the gradient penalty, all you need to do is add a regularization term to your loss function. What this regularization\n",
    "term does to your W loss function, is that it penalizes the critic when it's gradient norm is higher than one.\n",
    "I'll dive into what that means. The regularization term is as reg here, which I'll unfold shortly.\n",
    "Lambda is just a hyperparameter value of how much to weigh this regularization term against the main loss function.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "###### Impractically to check gradient at every possible point of the feature space\n",
    "###### Check on the Interpolation between real and fake\n",
    "\n",
    "<img src=\"images/wloss-1L-10.png\" width=420 height=200 />\n",
    "\n",
    "In order to check the critics gradient at every possible point of the feature space, that's virtually impossible or at\n",
    "least not practical. Instead with gradient penalty during implementation, of course, all you do is sample some points by\n",
    "interpolating between real and fake examples. For instance, you could sample an image with a set of reals and an image of\n",
    "the set of fakes, and you grab one of each and you can get an intermediate image by interpolating those two images using\n",
    "a random number epsilon. Epsilon here it could be a weight of 0.3, and here it would evaluate one minus epsilon would be 0.7.\n",
    "That would get you this random interpolated image that's in-between these two images. I'll call this random interpolated\n",
    "image X hat. It's on X hat that you want to get the critics gradient to be less than or equal to one.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"images/wloss-1L-11.png\" width=420 height=200 />\n",
    "\n",
    "The two here is just saying,\"I want the squared distance as opposed to perhaps the absolute value between them, penalizing\n",
    "values much more when they're further away from one.\" Specifically, that X hat is an intermediate image where it's weighted\n",
    "against the real and a fake using epsilon.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"images/wloss-1L-12.png\" width=420 height=200 />\n",
    "\n",
    "With this method, you're not strictly enforcing 1-L continuity, but you're just encouraging it. This has proven to work\n",
    "well and much better than weight clipping. The complete expression, the loss function that you use for training again with\n",
    "W loss ingredient penalty now has these two components. First, you approximate Earth Mover's distance with this main W\n",
    "loss component. This makes again less parental mode collapse and managed ingredients. The second part of this loss\n",
    "function is a regularization term that meets the condition for what the critic desires in order to make this main term\n",
    "valid. Of course, this is a soft constraint on making the critic one lipschitz continuous, but it has been shown to be\n",
    "very effective. Keeping the norm of the critic close to one almost everywhere is actually the technical term is almost\n",
    "anywhere.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"images/wloss-1L-13.png\" width=420 height=200 />\n",
    "\n",
    "Wrapping up in this video, I presented you with two ways of enforcing the critic to be one lipschitz continuous or 1L\n",
    "continuous, weight clipping as one and ingredient penalty as the other. Weight clipping might be problematic because\n",
    "you're strongly limiting the way the critic learns during training or you're being too soft, so there's a bit of\n",
    "hyperparameter tuning. Gradient penalty on the other hand, is a softer way to enforce one Lipschitz continuity.\n",
    "While it doesn't strictly enforce the critics gradient norm to be less than one at every point, it works better in\n",
    "practice than weight clipping.\n",
    "\n",
    "<img src=\"images/wloss-1L-14.png\" width=420 height=200 />\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-3068b003",
   "language": "python",
   "display_name": "PyCharm (mark)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}